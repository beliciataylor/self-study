{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38164bitac18278f1e914ec2bb6607d9943d2786",
   "display_name": "Python 3.8.1 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 2: Large-scale data analysis with spaCy\n",
    "\n",
    "This chapter will\n",
    "* extract specific information from large volumns of text\n",
    "* make most of spaCy's data structures\n",
    "* effectively combine statistical & rule-based approaches for text analysis\n",
    "\n",
    "**Sections** \n",
    "\n",
    "1. Data Structures (Part 1) \n",
    "2. Strings to hashes \n",
    "3. Vocab, hashes and lexemes \n",
    "4. Data Structures (Part 2) \n",
    "5. Creating a Doc \n",
    "6. Docs, spans and entities from scratch \n",
    "7. Data structures best practices \n",
    "8. Word vectors and semantic similarity \n",
    "9. Inspecting word vectors \n",
    "10. Comparing similarities \n",
    "11. Combining models and rules\n",
    "12. Debugging patterns (Part 1) \n",
    "13. Debugging patterns (Part 2) \n",
    "14. Efficient phrase matching \n",
    "15. Extracting countries and relationships"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Data Structures (Part 1): Vocab, Lexemes, and StringStore\n",
    "* Goal: Look at shared vocabulary and how spaCy deals with strings\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Shared vocab and string store (1)\n",
    "* `Vocab`: stores data share across multiple documents\n",
    "* To save memory, spaCy encodes all strings to **hash values**\n",
    "* Strings are only stored once in the `StringStore` via `nlp.vocab.strings`\n",
    "* String store: **lookup table** in both directions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "print(coffee_hash)\n",
    "print(coffee_string)"
   ]
  },
  {
   "source": [
    "* Hashes can't be reversed - that's why we need to provide the shared vocab"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "\n",
    "# Raises an error if we haven't seen the string before\n",
    "string = nlp.vocab.string[123456789]"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* spaCy stores all shared data in a vocabulary: the Vocab\n",
    "    - includes words but also labels schemes for tags and entities\n",
    "* to save memory, all strings are encoded to hash IDs\n",
    "    - if a word occurs more than once, we don't need to save it every time\n",
    "* instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store\n",
    "    - string store available as `nlp.vocab.strings`\n",
    "    - `nlp.vocab.strings` is a lookup table that works in both directions\n",
    "    - you can look up a string and get its hash\n",
    "    - you can also look up a hash to get its string value\n",
    "    - internally, spaCy only communicates in hash IDs\n",
    "* hash IDs can't be reversed\n",
    "    - if a word is not in vocabulary, there's no way to get its string, meaning we always need to pass around a shared vocab"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Shared vocab and string store (2)\n",
    "* Look up the string and hash in `nlp.vocab.strings`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hash value: 3197928453018144401\nstring value: coffee\n"
     ]
    }
   ],
   "source": [
    "# not in slide\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# in slide\n",
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "source": [
    "* the `doc` also exposes the vocab and strings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hash value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* to get the hash for a string, we can look it up in `nlp.vocab.strings`\n",
    "* to get the string representation of a hash, we can look up the hash\n",
    "* a `Doc` object also exposes its vocab and strings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Lexemes: entries in the vocabulary\n",
    "* a `Lexeme` object is an entry in the vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "source": [
    "* contains the **context-independent** information about a word\n",
    "    - word text: `lexeme.text` and `lexeme.orth` (the hash)\n",
    "    - lexical attributes like `lexeme.is_alpha`\n",
    "    - **not** context-dependent part-of-speech tags, dependencies or entity labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Notes**\n",
    "* lexemes are context-independent entries in the vocabulary\n",
    "* you can get a lexeme by looking up a string or a hash ID in the vocabulary\n",
    "* lexemes expose attributes, just like tokens\n",
    "    - they hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters\n",
    "* lexemes don't have part-of-speech tags, dependencies or entity labels\n",
    "    - those depend on the context"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Vocab, hashes and lexemes\n",
    "(note: slide has a diagram)\n",
    "\n",
    "* the `Doc` contains words in contexts\n",
    "    - EX. the tokens \"I\", \"love\", and \"coffee\"\n",
    "    - contains their part-of-speech tags and dependencies\n",
    "* each token refers to a lexeme, which knows the word's hash ID\n",
    "* to get the string representation of the word, spaCy looks up the hash in the string store"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Strings to hashes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5439657043933447811\ncat\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "380\nPERSON\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "source": [
    "## 3. Vocab, hashes and lexemes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2644858412616767388\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '2644858412616767388'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f44251105970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Look up the ID for \"Bowie\" in the vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbowie_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '2644858412616767388'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "# Why does this code throw an error?\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create an English and German nlp object\n",
    "nlp = English()\n",
    "nlp_de = German()\n",
    "\n",
    "# Get the ID for the string 'Bowie'\n",
    "bowie_id = nlp.vocab.strings[\"Bowie\"]\n",
    "print(bowie_id)\n",
    "\n",
    "# Look up the ID for \"Bowie\" in the vocab\n",
    "print(nlp_de.vocab.strings[bowie_id])"
   ]
  },
  {
   "source": [
    "(**x**) The string `\"Bowie\"` isn't in the German vocab, so the hash can't be resolved in the string store\n",
    "\n",
    "() `\"Bowie\"` is not a regular word in the English or German dictionary, so it can't be hashed\n",
    "\n",
    "() `nlp_de` is not a valid name. The vocab can only be shared if the `nlp` objects have the same name\n",
    "\n",
    "Explanation: Hashes can't be reversed. To prevent this problem, add the word to the new vocab by processing a text or looking up the string, or use the same vocab to resolve the hash back to a string."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. Data Structures (Part 2): Doc, Span and Token"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The Doc Object"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* the `Doc` is one of the central data structures in spaCy\n",
    "    - it's created automatically when you process a text with an `nlp` object\n",
    "    - you can also instantiate the class manually\n",
    "* after creating the `nlp` object, we can import the `Doc` class from `spacy.tokens`\n",
    "* EX: creating a doc from 3 words\n",
    "    - spaces are a list of boolean values indicating whether the word is followed by a space\n",
    "    - every token includes that information\n",
    "* the `Doc` class takes three arguments\n",
    "    - the shared vocab\n",
    "    - the words\n",
    "    - the spaces"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The Span object (1)\n",
    "(note: slide has a diagram)\n",
    "* a `Span` is a slice of a doc consisting of one or more tokens\n",
    "* the `Span` takes at least three arguments\n",
    "    - the doc it refers to\n",
    "    - the start index of a span\n",
    "    - the end index of a span (which is exclusive!)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The Span object (2)\n",
    "* to create a `Span` manually, we can also import the class from `spacy.tokens`\n",
    "* we can then instantiate it with the doc and the span's start and end index, and an optional label argument\n",
    "* the `doc.ents` are writable, so we can add entites manually by overwriting it with a list of spans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Doc: Hello world!\nSpan: Hello world\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(\"Doc:\", doc)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "print(\"Span:\", span)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "source": [
    "### Best practices\n",
    "* `Doc` and `Span` are very powerful and hold references and relationships of words and sentences\n",
    "    - **convert result to strings as late as possible**\n",
    "    - **use token attributes if available**\n",
    "        - EX. `token.i` for the token index\n",
    "* Don't forget to pass in the shared `vocab`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**note**\n",
    "* `Doc` and `Span` are very powerful and optimized for performance\n",
    "    - give you access to all references and relationships of words and sentences\n",
    "* if your application needs to output strings, make sure to convert the doc as late as possible\n",
    "    - if you do it too early, you'll lose all the relationships between the tokens\n",
    "* to keep things consistent, try to use built-in token attributes whenever possible\n",
    "    - EX: `token.i` for the token index\n",
    "* don't forget to always pass in the shared vocab"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5. Creating a Doc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "source": [
    "## 6. Docs, spans and entities from scratch\n",
    "\n",
    "In this exercise, you'll create the `Doc` and `Span` objects manually, and update the named entities - just like spaCy does behind the scenes. A shared `nlp` object has already been created."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I like David Bowie\nDavid Bowie PERSON\n[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words, spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "source": [
    "## 7. Data structures best practices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 (did not run)\n",
    "# Why is this code bad?\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "source": [
    "Why is this code bad?\n",
    "\n",
    "ANSWER: It only uses lists of strings instead of native token attributes. This is often less efficient and can't express complex relationships.\n",
    "\n",
    "EXPLANATION: Always convert the results to strings as late as possible, and try to use native token attributes to keep things consistent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "source": [
    "Note about solution: If the doc ends in a proper noun, `doc[token.i + 1]` will fail. To make sure the code generalizes, you should first check if `token.i + 1 < len(doc)`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8. Word vectors and semantic similarity\n",
    "GOALS:\n",
    "\n",
    "1. learn how to use spaCy to predict how similar documents, spans or tokens are\n",
    "\n",
    "2. learn how to use word vectors and how to take advantage of them in your NLP application"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Comparing semantic similarity\n",
    "* `spaCy` can compare two objects and predict similarity\n",
    "* `Doc.similarity()`, `Span.similarity()` and `Token.similarity()`\n",
    "* Take another object and return a similarity score (`0` to `1`)\n",
    "* **Important**: needs a model that has word vectors included, for example:\n",
    "    - (x) `en_core_web_md` (medium model)\n",
    "    - (x) `en_core_web_lg` (large model)\n",
    "    - ( ) **NOT** `en_core_web_sm` (small model)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Notes**\n",
    "* spaCy can compare two objects and predict how similar they are (EX. documents, spans, or single tokens)\n",
    "* `Doc`, `Token`, and `Span` objects have a `.similarity` method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are\n",
    "* in order to use similarity, you need a larger spaCy model that has word vectors included\n",
    "    - EX. medium or large English model but NOT the small one\n",
    "    - if you want to use vectors, always go with a model that ends with \"md\" or \"lg\"\n",
    "* find more details in the [models documentation](https://spacy.io/models)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Similarity examples (1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8627204117787385\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7369546\n"
     ]
    }
   ],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "source": [
    "### Similarity examples (2)\n",
    "\n",
    "You can compare different types of objects as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.32531983166759537\n"
     ]
    }
   ],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6199092090831612\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "source": [
    "### How does spaCy predict similarity?\n",
    "* Similarity is determined using **word vectors**\n",
    "* Multi-dimensional meaning representations of words\n",
    "* Generated using an algorithm like [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)\n",
    "* Can be added to spaCy's statistical models\n",
    "* DEFAULT: cosine similarity, but can be adjusted\n",
    "* `Doc` and `Span` vectors default to average of token vectors\n",
    "* Short phrases are better than long documents with many irrelevant words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Notes**\n",
    "* Similarity is determined using word vectors\n",
    "    - multi-dimensional representations of meanings of words\n",
    "    - EX: Word2Vec, an algorithm that's often used to train word vectors from raw text\n",
    "* Vectors can be added to spaCy's statistical models\n",
    "* By default, the similarity returned by spaCy is the cosine similarity between two vectors, but can be adjusted if necessary\n",
    "* Vectors for objects consisting of several tokens, like the `Doc` and `Span`, default to the average of their token vectors\n",
    "    - also why you usually get more value out of shorter phrases with fewer irrelevant words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Word vectors in spaCy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* We can process a text and look ip a token's vector using the `.vector` attribute\n",
    "* The result is a 300-dimensional vector of the word \"banana\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Similarity depends on the application context\n",
    "* Useful for many applications: recommendation systems, flagging duplicates, etc.\n",
    "* There's no objective definition of \"similarity\"\n",
    "* Depends on the context and what application needs to do"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9501447503553421\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* Predicting similarity can be useful for many types of applications\n",
    "    - EX: to recommend a user similar texts based on the ones they have read\n",
    "    - EX: flag duplicate content like posts on an online platform\n",
    "* Important to keep in mind there's no objective definition of what's similar and what isn't\n",
    "    - Always depends on the context and what your application needs to do\n",
    "* EX: High similarity score in above example\n",
    "    - Score makes sense because both texts express sentiment about cats\n",
    "    - However, in different application context, you might want to consider the phrases as very *dissimilar* because they talk about opposite sentiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9. Inspecting word vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-2.2009e-01 -3.0322e-02 -7.9859e-02 -4.6279e-01 -3.8600e-01  3.6962e-01\n -7.7178e-01 -1.1529e-01  3.3601e-02  5.6573e-01 -2.4001e-01  4.1833e-01\n  1.5049e-01  3.5621e-01 -2.1508e-01 -4.2743e-01  8.1400e-02  3.3916e-01\n  2.1637e-01  1.4792e-01  4.5811e-01  2.0966e-01 -3.5706e-01  2.3800e-01\n  2.7971e-02 -8.4538e-01  4.1917e-01 -3.9181e-01  4.0434e-04 -1.0662e+00\n  1.4591e-01  1.4643e-03  5.1277e-01  2.6072e-01  8.3785e-02  3.0340e-01\n  1.8579e-01  5.9999e-02 -4.0270e-01  5.0888e-01 -1.1358e-01 -2.8854e-01\n -2.7068e-01  1.1017e-02 -2.2217e-01  6.9076e-01  3.6459e-02  3.0394e-01\n  5.6989e-02  2.2733e-01 -9.9473e-02  1.5165e-01  1.3540e-01 -2.4965e-01\n  9.8078e-01 -8.0492e-01  1.9326e-01  3.1128e-01  5.5390e-02 -4.2423e-01\n -1.4082e-02  1.2708e-01  1.8868e-01  5.9777e-02 -2.2215e-01 -8.3950e-01\n  9.1987e-02  1.0180e-01 -3.1299e-01  5.5083e-01 -3.0717e-01  4.4201e-01\n  1.2666e-01  3.7643e-01  3.2333e-01  9.5673e-02  2.5083e-01 -6.4049e-02\n  4.2143e-01 -1.9375e-01  3.8026e-01  7.0883e-03 -2.0371e-01  1.5402e-01\n -3.7877e-03 -2.9396e-01  9.6518e-01  2.0068e-01 -5.6572e-01 -2.2581e-01\n  3.2251e-01 -3.4634e-01  2.7064e-01 -2.0687e-01 -4.7229e-01  3.1704e-01\n -3.4665e-01 -2.5188e-01 -1.1201e-01 -3.3937e-01  3.1518e-01 -3.2221e-01\n -2.4530e-01 -7.1571e-02 -4.3971e-01 -1.2070e+00  3.3365e-01 -5.8208e-02\n  8.0899e-01  4.2335e-01  3.8678e-01 -6.0797e-01 -7.3760e-01 -2.0547e-01\n -1.7499e-01 -3.7842e-03  2.1930e-01 -5.2486e-02  3.4869e-01  4.3852e-01\n -3.4471e-01  2.8910e-01  7.2554e-02 -4.8625e-01 -3.8390e-01 -4.4760e-01\n  4.3278e-01 -2.7128e-03 -9.0067e-01 -3.0819e-02 -3.8630e-01 -8.0798e-02\n -1.6243e-01  2.8830e-01 -2.6349e-01  1.7628e-01  3.5958e-01  5.7672e-01\n -5.4624e-01  3.8555e-02 -2.0182e+00  3.2916e-01  3.4672e-01  1.5398e-01\n -4.3446e-01 -4.1428e-02 -6.9588e-02  5.1513e-01 -1.3489e-01 -5.7239e-02\n  4.9241e-01  1.8643e-01  3.8596e-01 -3.7329e-02 -5.4216e-01 -1.8152e-01\n  4.3110e-01 -4.6967e-01  6.6801e-02  5.0323e-01 -2.4059e-01  3.6742e-01\n  2.9300e-01 -8.7883e-02 -4.7940e-01 -4.3431e-02 -2.6137e-01 -6.2658e-01\n  1.1446e-01  2.7682e-01  3.4800e-01  5.0018e-01  1.4269e-01 -3.3545e-01\n -3.9712e-01 -3.3121e-01 -3.4434e-01 -4.1627e-01 -3.5707e-03 -6.2350e-01\n  3.7794e-01 -1.6765e-01 -4.1954e-01 -3.3134e-01  3.1232e-01 -3.9494e-01\n -4.6921e-03 -4.8884e-01 -2.2059e-02 -2.6174e-01  1.7937e-01  3.6628e-01\n  5.8971e-02 -3.5991e-01 -4.4393e-01 -1.1890e-01  3.3487e-01  3.6505e-02\n -3.2788e-01  3.3425e-01 -5.6361e-01 -1.1190e-01  5.3770e-01  2.0311e-01\n  1.5110e-01  1.0623e-02  3.3401e-01  4.6084e-01  5.6293e-01 -7.5432e-02\n  5.4813e-01  1.9395e-01 -2.6265e-01 -3.1699e-01 -8.1778e-01  5.8169e-02\n -5.7866e-02 -1.1781e-01 -5.8742e-02 -1.4092e-01 -9.9394e-01 -9.4532e-02\n  2.3503e-01 -4.9027e-01  8.5832e-01  1.1540e-01 -1.5049e-01  1.9065e-01\n -2.6705e-01  2.5326e-01 -6.7579e-01 -1.0633e-02 -5.5158e-02 -3.1004e-01\n -5.8036e-02 -1.7200e-01  1.3298e-01 -3.2899e-01 -7.5481e-02  2.9425e-02\n -3.2949e-01 -1.8691e-01 -9.5323e-01 -3.5468e-01 -3.3162e-01  5.6441e-02\n  2.1790e-02  1.7182e-01 -4.4267e-01  6.9765e-01 -2.6876e-01  1.1659e-01\n -1.6584e-01  3.8296e-01  2.9109e-01  3.6318e-01  3.6961e-01  1.6305e-01\n  1.8152e-01  2.2453e-01  3.9866e-02 -3.7607e-02 -3.6089e-01  7.0818e-02\n -2.1509e-01  3.6551e-01 -5.1603e-01 -5.8102e-03 -4.8320e-01 -2.5068e-01\n -5.2062e-02 -2.0828e-01  2.9060e-01  2.2084e-02 -6.8123e-01  4.2063e-01\n  9.5973e-02  8.1720e-01 -1.5241e-01  6.2994e-01  2.6449e-01 -1.3516e-01\n  3.2450e-01  3.0503e-01  1.2357e-01  1.5107e-01  2.8327e-01 -3.3838e-01\n  4.6106e-02 -1.2361e-01  1.4516e-01 -2.7947e-02  2.6231e-02 -5.9591e-01\n -4.4183e-01  7.8440e-01 -3.4375e-02 -1.3928e+00  3.5248e-01  6.5220e-01]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_md model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "source": [
    "## 10. Comparing similarities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8789265574516525\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.22325331\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.75173926\n"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[-4:-1]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "source": [
    "The similarities are not always this conclusive. Once you're getting serious about developing NLP applications that leverage semantic similarity, you might want to train vectors on your own data, or tweak the similarity algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 11. Combining models and rules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Statistical predictions vs. rules\n",
    "**Notes**\n",
    "* statistical models are useful if your application needs to be able to generalize based on a few examples\n",
    "    - EX. detecting product or person names usually benefits from a statistical model\n",
    "    - i.e. instead of providing a list of all person names ever, your application will be able to predict whether a span of tokens is a person name\n",
    "    - similarly, you can predict dependency labels to find subject/object relationships\n",
    "* to do this, you would use spaCy's entity recognizer, dependency parser or part-of-speech tagger\n",
    "* rule-based approaches come in handy if there's a more or less finite number of instances you want to find\n",
    "    - EX: all countries or cities of the world, drug names or even dog breeds\n",
    "    - in spaCy you can achieve this with custom tokenization rules as well as the matcher and phrase matcher"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Table\n",
    "* use cases\n",
    "    - statistical models: applicatoin needs to generalize based on examples\n",
    "    - rule-based systems: dictionary with finite number of examples\n",
    "* real-world examples\n",
    "    - statistical models: product names, person names, subject/object relationships\n",
    "    - rule-based systems: countries of the world, citires, drug names, dog breeds\n",
    "* spaCy features\n",
    "    - statistical models: entity recognizer, dependency parser, part-of-speech tagger\n",
    "    - rule-based systems: tokenizer, `Matcher`, `PhraseMatcher`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recap: Rule-based Matching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\", None, pattern)\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "matcher.add(\"VERY_HAPPY\", None, pattern)\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "source": [
    "### Adding statistical predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matched span: Golden Retriever\nRoot token: Retriever\nRoot head token: have\nPrevious token: a DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOG\", None, [{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "source": [
    "**Note**\n",
    "* Above EX: \"golden retriever\"\n",
    "* If we iterate over the matches returned by the matcher, we can get the match ID and the start and end index of the matched span & find out more about it\n",
    "    - `Span` objects give us access to the original document and all other token attributes and linguistic features predicted by the model\n",
    "* EX: we can get the span's root token\n",
    "    - if span consists of more than one token, this will be the token that decides the category of the phrase\n",
    "    - EX: the root of \"Golden Retriever\" is \"Retriever\"\n",
    "* we can also find the head token of the root\n",
    "    - this is the syntactic \"parent\" that governs the phrase i.e. the verb \"have\"\n",
    "* finally, we can look at the previous token and its attributes\n",
    "    - it's determiner, the article \"a\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Efficient phrase matching (1)\n",
    "* `PhraseMatcher` like regular expressions or keyword search - but with access to the tokens!\n",
    "* takes `Doc` object as patterns\n",
    "* more efficient and faster than the `Matcher`\n",
    "* great for matching large word lists\n",
    "\n",
    "**Notes**\n",
    "* the phrase matcher is another helpful tool to find sequences of words in your data\n",
    "* it performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context\n",
    "* it takes `Doc` objects as patterns AND it's also really fast\n",
    "* this makes it very useful for matching large dictionaries and word lists on large volumns of text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Efficient phrase matching (2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "source": [
    "* the phrase matcher can be imported from `spacy.matcher` and follows the same API as the regular matcher\n",
    "* instead of a list of dictionaries, we pass in a `Doc` object as the pattern\n",
    "* we can then iterate over the matches in the text, which gives us the match ID, and the start and end of the match\n",
    "* this lets us create a `Span` object for the matched tokens \"Golden Retriever\" to analyze it in context"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 12. Debugging patterns (1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why does this pattern not match the tokens \"Silicon Valley\" in the `doc`?\n",
    "pattern = [{\"LOWER\": \"silicon\"}, {\"TEXT\": \" \"}, {\"LOWER\": \"valley\"}]\n",
    "\n",
    "doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")"
   ]
  },
  {
   "source": [
    "( ) The tokens \"Silicon\" and \"Valley\" are not lowercase, so the \"LOWER\" attribute won't match\n",
    "- **Incorrect**: The \"LOWER\" attribute in the pattern describes tokens whose lowercase form matches the given value\n",
    "\n",
    "(x) The tokenizer doesn't create tokens for single spaces, so there's no token with the value \" \" in between.\n",
    "\n",
    "( ) The tokens are missing an operator \"OP\" to indicate that they should be matched exactly once.\n",
    "\n",
    "**EXPLANATION**: The tokenizer already takes care of splitting off whitespace and each dictionary in the pattern describes one token"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 13. Debugging patterns (2)\n",
    "\n",
    "Both patterns in this exercise contain mistakes and won't match as expected. Can you fix them? If you get stuck, try printing the tokens in the `doc` to see how the text will be split and adjust the pattern so that each dictionary represents one token>\n",
    "\n",
    "1. Edit `pattern1` so that it correctly matches all case-insensitive mentions of `\"Amazon\"` plus a title-cased proper noun\n",
    "\n",
    "2. Edit `pattern2` so that it correctly matches all case-insensitive mentions of `\"ad-free\"`, plus the following noun.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL CODE\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"Amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad-free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PATTERN1 Amazon Prime\nPATTERN2 ad-free viewing\nPATTERN1 Amazon Prime\nPATTERN2 ad-free viewing\nPATTERN2 ad-free viewing\nPATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "# NEW CODE\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "source": [
    "**Correct!**: For the token '-', you can match on the attribute 'TEXT', 'LOWER', or even 'SHAPE'. All of those are correct. As you can see, paying close attention to the tokenization is very important when working with the token-based 'Matcher'. Sometimes it's much easier to just match exact strings instead and use the 'PhraseMatcher'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 14. Efficient phrase matching\n",
    "\n",
    "Sometimes it's more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things - like all countries of the world. We already have a list of countries, so let's use this as the basis of our information extraction script. A list of string names is available as the variable `COUNTRIES`.\n",
    "\n",
    "* Import the `PhraseMatcher` and initialize it with the shared `vocab` as the variable `matcher`\n",
    "* Add the phrase patterns and call the matcher on the `doc`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "source": [
    "## 15. Extracting countries and relationships\n",
    "\n",
    "Now we'll use the country matcher in the previous exercise on a longer text, analyze the syntax and update the document's entities with the matches countries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (did not run)\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"exercises/en/country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Create a doc and reset existing entities\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  }
 ]
}
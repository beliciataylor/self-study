{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 3: Processing Pipelines\n",
    "\n",
    "This chapter will show you everything you need to know about spaCy's processing pipeline. You'll learn what goes on under the hood when you process a text, how to write your own components and add them to the pipeline, and how to use custom attributes to add your own metadata to the documents, spans and tokens.\n",
    "\n",
    "**Sections**\n",
    "\n",
    "1. Processing pipelines \n",
    "2. What happens when you call nlp? \n",
    "3. Inspecting the pipeline \n",
    "4. Custom pipeline components \n",
    "5. Use cases for custom components \n",
    "6. Simple components \n",
    "7. Complex components \n",
    "8. Extension attributes \n",
    "9. Setting extension attributes (Part 1) \n",
    "10. Setting extension attributes (Part 2) \n",
    "11. Entities and extensions \n",
    "12. Components with extensions \n",
    "13. Scaling and performance \n",
    "14. Processing streams \n",
    "15. Processing data with context \n",
    "16. Selective processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Processing pipelines\n",
    "\n",
    "* processing pipelines: a series of functions applied to a doc to add attributes like part-of-speech tags, dependency labels, or named entities\n",
    "* this lesson: learn about the pipeline components provided by spaCy, and what happens behind the scenes when you call nlp on a string of text\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### What happens when you call nlp?\n",
    "* First, the tokenizer is applied to turn the string of text into a `Doc` object\n",
    "* Next, a series of pipeline components is applied to the doc in order\n",
    "    - In this case, the tagger, then the parser, then the entity recognizer\n",
    "* Finally, the processed doc is returned, so you can work with it\n",
    "\n",
    "Text --> nlp \\[ tokenizer -> tagger -> parser -> ner -> ... \\] --> Doc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Built-in pipeline components\n",
    "\n",
    "* tagger\n",
    "    - description: part-of-speech\n",
    "    - creates: `Token.tag`, `Token.pos`\n",
    "* parser\n",
    "    - description: Dependency parser\n",
    "    - creates: `Token.dep`, `Token.head`, `Doc.sents`, `Doc.noun_chunks`\n",
    "* ner\n",
    "    - description: named entity recognizer\n",
    "    - creates: `Doc.ents`, `Token.ent_iob`, `Token.ent_type`\n",
    "* textcat\n",
    "    - description: text classifier\n",
    "    - creates: `Doc.cats`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Notes**\n",
    "* spaCy ships with the following built-in pipeline components\n",
    "* the part-of-speech tagger\n",
    "    - sets the `token.tag` and `token.pos` attributes\n",
    "* the dependency parser\n",
    "    - adds the `token.dep` and `token.head` attributes\n",
    "    - is also responsible for detecting sentences and base noun phrases also known as noun chunks\n",
    "* the named entity recognizer\n",
    "    - adds the detected entities to the `doc.ents` property\n",
    "    - also sets entity type attributes on the tokens that indicate if a token is part of an entity or not\n",
    "* the text classifier\n",
    "    - sets category labels that apply to the whole text\n",
    "    - adds them to the `doc.cats` property\n",
    "    - since text categories are always very specific, the text classifier is not included in any of the pre-trained models by default, but you can use it to train your own system"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Under the hood\n",
    "\n",
    "* pipeline defined in model's `meta.json` in order\n",
    "* built-in components need binary data to make predictions\n",
    "\n",
    "**Notes**\n",
    "* all models you can load into spaCy include several files and a `meta.json`\n",
    "* the meta defines things like the language and pipeline\n",
    "    - this tells spaCy which components to instantiate\n",
    "* the built-in components that make predictions also need binary data\n",
    "    - the data included in the model package and loaded into the component when you load the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Pipeline attributes\n",
    "* `nlp.pipe_names`: list of pipeline component names"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "source": [
    "* `nlp.pipeline`: list of (name, component) tuples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('tagger', <spacy.pipeline.pipes.Tagger object at 0x7fa09e80bd50>)\n('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7fa09e327de0>)\n('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7fa09e327d70>)\n"
     ]
    }
   ],
   "source": [
    "for pipeline in nlp.pipeline:\n",
    "    print(pipeline)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* to see the names of the pipeline components present in the current nlp object, you can use the `nlp.pipe_names` attribute\n",
    "* for a list of component name and component function tuples, you can use the `nlp.pipeline` attribute\n",
    "* the component functions are the functions applied to the doc to process it and set attributes\n",
    "    - EX: part-of-speech tags or named entities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. What happens when you call nlp?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does spaCy do when you call `nlp` on a string of text?\n",
    "# (did not run)\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "source": [
    "( ) Run the tagger, parser, and entity recognizer and then the tokenizer\n",
    "\n",
    "(X) Tokenize the text and apply each pipeline component in order\n",
    "\n",
    "( ) Connect to the spaCy server to compute the result and return it\n",
    "\n",
    "( ) Initialize the language, add the pipeline and load in the binary model weights\n",
    "\n",
    "**Correct!**: The tokenizer turns a string of text into a `Doc` object. spaCy then applied every component in the pipeline on the document, in order."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Inspecting the pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tagger', 'parser', 'ner']\n[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7fa08521b690>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7fa088500670>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7fa0885008a0>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "source": [
    "## 4. Custom pipeline components\n",
    "* custom pipeline components let you add your own function to the spaCy pipeline that is executed when you call the `nlp` object on a text\n",
    "    - EX: to modify the doc and add more data to it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Why custom components?\n",
    "* make a function execute automatically when you call `nlp`\n",
    "* add your own metadata to documents and tokens\n",
    "* updating built-in attributes like `doc.ents`\n",
    "\n",
    "**Notes**\n",
    "* after the text is tokenized and a `Doc` object has been created, pipeline components are applied in order\n",
    "* spaCy supports a range of built-in components, but also lets you define your own\n",
    "* custom components are executed automatically when you call the `nlp` object on a text\n",
    "* they're especially useful for adding your own custom metadata to documents and tokens\n",
    "* you can also use them to update built-in attributes like the named entity spans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Anatomy of a component (1)\n",
    "* function that takes a `doc`, modifies it and returns it\n",
    "* can be added using the `nlp.add_pipe` method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def custom_component(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* fundamentally: a pipeline component is a function or callable that takes a doc, modifies it, and returns it, so it can be processed by the next component in the pipeline\n",
    "* components can be added to the pipeline using the `nlp.add_pipe` method\n",
    "* the method takes at least one argument: the component function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Anatomy of a component (2)\n",
    "\n",
    "* `last`\n",
    "    - description: if `True`, add last\n",
    "    - example: `nlp.add_pipe(component, last=True)\n",
    "    - default option\n",
    "* `first`\n",
    "    - description: if `True`, add first\n",
    "    - example: `nlp.add_pipe(component, first=True)`\n",
    "    - right after tokenizer\n",
    "* `before`\n",
    "    - description: add before component\n",
    "    - example: `nlp.add_pipe(component, before=\"ner\")`\n",
    "* `after`\n",
    "    - description: add after component\n",
    "    - example: `nlp.add_pipe(component, after=\"tagger\")`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Example: a single component (1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* don't forget to return the doc so it can be processed by the next component in the pipeline\n",
    "* the doc created by the tokenizer is passed through all components, so it's important that they all return the modified doc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Example: a simple component (2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "source": [
    "## 5. Use cases for custom components\n",
    "\n",
    "Which of these problems can be solved by custom pipeline components? Choose all the apply!\n",
    "\n",
    "1. Updating the pre-trained models and improving their predictions \n",
    "2. Computing your own values based on tokens and their attributes (X)\n",
    "3. Adding named entities, for example based on a dictionary (X)\n",
    "4. Implementing support for an additional language\n",
    "\n",
    "**Explanation**: Custom components are great for adding custom values to documents, tokens and spans, and customizing the `doc.ents`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6. Simple components\n",
    "\n",
    "The example shows a custom component that prints the number of tokens in a document. Can you complete it?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['length_component', 'tagger', 'parser', 'ner']\nThis document is 4 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(length_component, first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"I love this movie\")"
   ]
  },
  {
   "source": [
    "## 7. Complex components\n",
    "\n",
    "In this exercise, you'll be writing a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the `doc.ents`. A `PhraseMatcher` with the animal patterns has already been created as a variable `matcher`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n['tagger', 'parser', 'ner', 'animal_component']\n[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(animal_component, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "source": [
    "## 8. Extension attributes\n",
    "\n",
    "We'll learn how to add custom attributes to the `Doc`, `Token`, and `Span` objects to store custom data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Setting custom attributes\n",
    "* Add custom metadata to documents, tokens, and spans\n",
    "* Accessible via the `._` property"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (did not run)\n",
    "doc._.title = \"My document\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False"
   ]
  },
  {
   "source": [
    "* Registered on the global `Doc`, `Token`, or `Span` using the `set_extension` method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extensions on the Doc, Token, and Span\n",
    "Doc.set_extension(\"title\", default=None)\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "Span.set_extension(\"has_color\", default=False)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* custom attributes let you add any metadata to docs, tokens, and spans\n",
    "    - the data can be added once, or it can be computed dynamically\n",
    "    - available via the `._` (dot underscore) property; makes it clear that they were added by the user and not built into spaCy like `token.text`\n",
    "    - need to be registered on the global `Doc`, `Token`, and `Span` classes you can import from `spacy.tokens`\n",
    "* to register a custom attribute on `Doc`, `Token`, and `Span`, you can use the `set_extension` method\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Extension attribute types\n",
    "1. Attribute extensions \n",
    "2. Property extensions \n",
    "3. Method extensions "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Attribute extensions\n",
    "* Set a default value that can be overwritten"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Set extension on the Token with default value\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "# Overwritten extension attribute value\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* attribute extensions set a default value that can be overwritten\n",
    "* EX: a custom `is_color` attribute on the token that defaults to `False`\n",
    "* on individual tokens, its value can be changed by overwriting it\n",
    "    - EX: True for the token \"blue\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Property extensions (1)\n",
    "* define a getter and an optional setter function\n",
    "* getter only called when you _retrieve_ the attribute value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extensions on the Token with getter\n",
    "Token.set_extension(\"is_color\", getter=get_is_color, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* Property extensions work like properties in Python; they can define a getter function and an optional setter\n",
    "    - the getter function is only called when you retrieve the attribute\n",
    "    - this lets you compute the value dynamically & even take other custom attributes into account\n",
    "* getter functions take one argument: the object i.e. the token\n",
    "    - EX: the function returns whether the token text is in our list of colors\n",
    "* we can then provide the function via the `getter` keyword argument when we register the extension\n",
    "* the token \"blue\" now returns `True` for `._.is_color`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Property extension (2)\n",
    "* `Span` extensions should almost always use a getter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True - sky is blue\nFalse - The sky\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = [\"red\", \"yellow\", \"blue\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension(\"has_color\", getter=get_has_color)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* if you want to set extension attributes on a span, you almost always want to use a property extension with a getter\n",
    "* otherwise, you'd have to update every possible span ever by hand to set all the values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Method extensions\n",
    "* Assign a **function** that becomes available as an object method\n",
    "* Lets you pass **arguments** to the extension function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True - blue\nFalse - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token(\"blue\"), \"- blue\")\n",
    "print(doc._.has_token(\"cloud\"), \"- cloud\")"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* method extensions make the extension attribute a callable method\n",
    "* you can then pass one or more arguments to it, and compute attribute values dynamically\n",
    "    - EX: based on a certain argument or setting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9. Setting extension attributes (1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reversed: llA\nreversed: snoitazilareneg\nreversed: era\nreversed: eslaf\nreversed: ,\nreversed: gnidulcni\nreversed: siht\nreversed: eno\nreversed: .\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "source": [
    "## 10. Setting extension attributes (2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension \"has_number\" with the getter get_has_number\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "source": [
    "## 11. Entities and extensions\n",
    "\n",
    "In this exercise, you'll combine custom extension attributes with the model's predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fifty years None\nDavid Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "source": [
    "## 12. Components with extensions\n",
    "\n",
    "Extension attributes are especially powerful if they're combined with custom pipeline components. In this exercise, you'll write a pipeline component that finds country names and a custom extension attribute that returns a country's capital, if available."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (did not run)\n",
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"exercises/en/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(countries_component)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "source": [
    "## 13. Scaling and performing\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Processing large volumes of text\n",
    "* Use `nlp.pipe` method\n",
    "* Processes texts as a stream, yields `Doc` objects\n",
    "* Much faster than calling `nlp` on each text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (did not use)\n",
    "# BAD\n",
    "docs = [nlp(text) for text in LOTS_OF_TEXTS]\n",
    "\n",
    "# GOOD\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXT))"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* if you need to process a lot of texts and create a lot of `Doc` objects in a row, the `nlp.pipe` method can speed this up significantly\n",
    "* it processes the texts as a stream and yields `Doc` objects\n",
    "* it's much faster than just calling nlp on each text because it baches up the texts\n",
    "* `nlp.pipe` is a generator that yields `Doc` objects, so in order to get a list of docs, remember to call the `list` method around it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Passing in context (1)\n",
    "* Setting as `as_tuples=True` on `nlp.pipe` lets you pass in `(text, context)` tuples\n",
    "* Yields `(doc, context)` tuples\n",
    "* Useful for associating metadata with the `doc`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is a text 15\nAnd another text 16\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16})\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"])"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* `nlp.pipe` also supports passing in tuples of text/context if you set `as_tuple` to `True`\n",
    "* the method will then yield doc / context tuples\n",
    "* this is useful for passing in additional metadata, like an ID associated with the text, or a page number"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Passing in context (2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"page_number\", default=None)\n",
    "\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16})\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.page_number = context[\"page_number\"]"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* you can even add the context metadata to custom attributes\n",
    "* EX: we're registering two extensions, `id` and `page_number`, which default to `None`\n",
    "* after processing the text and passing through the context, we can overwrite the doc extensions with our context metadata"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Using only the tokenizer (1)\n",
    "\n",
    "text --> nlp \\[ tokenizer -> tagger -> parser -> ner -> ... \\] --> Doc\n",
    "\n",
    "* don't run the whole pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Notes**\n",
    "* another common scenario: sometimes you already have a model loaded to do other processing, but you only need the tokenizer for one particular text\n",
    "* Running the whole pipeline is unnecessarily slow because you'll be getting a bunch of predictions from the model that you don't need"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Using only the tokenizer (2)\n",
    "* Use `nlp.make_doc` to turn a text into a `Doc` object"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (did not run)\n",
    "# BAD\n",
    "doc = nlp(\"Hello world\")\n",
    "\n",
    "# GOOD\n",
    "doc = nlp.make_doc(\"Hello world\")"
   ]
  },
  {
   "source": [
    "**Notes**\n",
    "* if you only need a tokenized `Doc` object you can use the `nlp.make_doc` method instead, which takes a text and returns a doc\n",
    "* this is also how spaCy does it behind the scenes: `nlp.make_doc` turns the text into a doc before the pipeline components are called"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Disabling pipeline components\n",
    "* Use `nlp.disable_pipes` to temporarily disable one or more pipes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (did not run)\n",
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "source": [
    "* Restores them after the `with` block\n",
    "* Only runs the remaining components\n",
    "\n",
    "**Notes**\n",
    "* spaCy also allows you to temporarily disable pipeline components using the `nlp.disable_pipes` context manager\n",
    "* it takes a variable number of arguments, the string names of the pipeline components to disable\n",
    "    - EX: if you only want to use the entity recognizer to process document, you can temporarily disable the tagger and parser\n",
    "* after the `with` block, the disabled pipeline components are automatically restored\n",
    "* in the `with` block, spaCy will only run the remaining components"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 14. Processing streams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# (do not run)\n",
    "# OLD CODE\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for text in TEXTS:\n",
    "    doc = nlp(text)\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "# NEW CODE\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "# (do not run)\n",
    "# OLD CODE\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = [nlp(text) for text in TEXTS]\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "# NEW CODE\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
    "    TEXTS = json.loads(f.read())\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "# OLD CODE\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = [nlp(person) for person in people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))"
   ]
  },
  {
   "source": [
    "## 15. Processing data with context"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (do not run)\n",
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open(\"exercises/en/bookquotes.json\", encoding=\"utf8\") as f:\n",
    "    DATA = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Doc extension \"author\" (default None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "# Register the Doc extension \"book\" (default None)\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(f\"{doc.text}\\n â€” '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "source": [
    "## 16. Selective processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}